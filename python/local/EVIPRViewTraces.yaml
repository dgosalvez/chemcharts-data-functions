id: 45c4c595-281d-a218-e5f3-78a35e87c80d
name: EVIPR View Traces
description: Unpacks raw .evpr files and displays traces
category: Vertex
version: 0.0.1
serviceName: Script
serviceUri: glysade.python
executorId: Glysade.CPythonDataFxn
inputFields:
- control:
    id: vp3Files
    label: Select column with file paths to process
    type: columnselect
    multi: !!bool false
    tooltip: Column should hold a list of .evpr file paths
    validationRules:
    - type: required
      message: ''
    filters:
    - dataType: string
      contentType: []
  request:
    id: vp3Files
    dataType: string
    selectorType: column
- control:
    id: vp3FileNames
    label: Select column with file names to process
    type: columnselect
    multi: !!bool false
    validationRules:
    - type: required
      message: ''
  request:
    id: vp3FileNames
    dataType: string
    selectorType: column
- control:
    id: scaleReferenceWellType
    label: Scale Reference Well Type
    type: text
    validationRules:
    - type: required
      message: ''
  request:
    id: scaleReferenceWellType
    dataType: string
    data: Stimulator Control
- control:
    id: centralReferenceWellType
    label: Central Reference Well Type
    type: text
    validationRules:
    - type: required
      message: ''
  request:
    id: centralReferenceWellType
    dataType: string
    data: Neutral Control
- control:
    id: peakRegions
    label: Peak Regions
    type: textarea
    validationRules:
    - type: required
      message: ''
  request:
    id: peakRegions
    dataType: string
    data: "{'R1':(19,79,'AVG'),'R2':(89,109,'MAX'),'R3':(119,129,'MAX'),'R4':(139,149 ,'MAX'),'R5':(159,169,'MAX'),'R6':(179,189,'MAX')}"
- control:
    id: peakCountNormalizationRegion
    label: Normalization Region
    type: text
    validationRules:
    - type: required
      message: ''
  request:
    id: peakCountNormalizationRegion
    dataType: string
    data: R1
- control:
    id: thresholdComputationRegion
    label: Threshold Computation Region
    type: text
    validationRules:
    - type: required
      message: ''
  request:
    id: thresholdComputationRegion
    dataType: string
    data: R2
- control:
    id: threshold_val
    label: Threshold
    type: text
    validationRules:
    - type: required
      message: ''
    - type: numeric
      message: ''
  request:
    id: threshold_val
    dataType: integer
    data: !!int 50
- control:
    id: differenceMethod
    label: Use Difference Method
    type: checkbox
  request:
    id: differenceMethod
    dataType: boolean
    data: !!bool true
- control:
    id: differenceMethodType
    label: Difference Method
    type: select
    multi: !!bool false
    options:
    - text: Pre
      value: pre
    validationRules:
    - type: required
      message: ''
  request:
    id: differenceMethodType
    dataType: string
    data: pre
- control:
    id: differenceMethodCalculation
    label: Method
    type: select
    multi: !!bool false
    options:
    - text: AVG
      value: AVG
    - text: MEAN
      value: MEAN
    - text: Geometric Mean
      value: GEO
    validationRules:
    - type: required
      message: ''
  request:
    id: differenceMethodCalculation
    dataType: string
    data: AVG
- control:
    id: differenceMethodNumPoints
    label: '# Points'
    type: text
    validationRules:
    - type: required
      message: ''
  request:
    id: differenceMethodNumPoints
    dataType: integer
    data: !!int 20
updateBehavior: manual
maximumOutputColumns: !!int 10
maximumOutputTables: !!int 5
chemistryFunction: !!bool false
script: |
  import io
  import itertools
  import statistics
  import struct
  from enum import Enum
  from urllib.request import Request, urlopen

  import numpy as np
  from df.data_transfer import DataFunctionRequest, DataFunctionResponse, DataType, ColumnData, \
      string_input_field, integer_input_field, boolean_input_field, TableData

  snb_api_key = "OBtnuLFItrL7g87qcrRquzwlwEaMOUoWloJokHFUz+KYEm1+ryi28ywDssCJByfPGrVf5w=="


  def execute(request: DataFunctionRequest) -> DataFunctionResponse:
      column_id = string_input_field(request, 'vp3Files')
      evp3_urls = request.inputColumns[column_id]
      column_id = string_input_field(request, 'vp3FileNames')
      evp3_names = request.inputColumns[column_id]
      evp3_files = {evp3_names.values[i]: evp3_urls.values[i] for i in range(len(evp3_names.values))}
      scale_reference_well_type = string_input_field(request, 'scaleReferenceWellType')
      central_reference_well_type = string_input_field(request, 'centralReferenceWellType')
      peak_regions = ast.literal_eval(string_input_field(request, 'peakRegions'))
      threshold_val = integer_input_field(request, 'threshold_val')
      peak_count_normalization_region = string_input_field(request, 'peakCountNormalizationRegion')
      threshold_computation_region = string_input_field(request, 'thresholdComputationRegion')
      difference_method = boolean_input_field(request, 'differenceMethod')
      difference_method_type = string_input_field(request, 'differenceMethodType')
      difference_method_calculation = string_input_field(request, 'differenceMethodCalculation')
      difference_method_num_points = integer_input_field(request, 'differenceMethodNumPoints')

      well_type_indices = make_plate_map(scale_reference_well_type, central_reference_well_type)
      scale_ref_well_type_indices = [i for i, wt in well_type_indices if wt == scale_reference_well_type]
      central_ref_well_type_indices = [i for i, wt in well_type_indices if wt == central_reference_well_type]

      output_tables = []
      for fileName, url in evp3_files.items():
          # read the file into colscans
          # out_file_name = "1p8_1p7 MC WC1 KB_BC_VV368247_2022-09-26_1017AM.vpr3"
          # file_name = "https://sld-share.s3.amazonaws.com/1p8_1p7+MC+WC1+KB_BC_VV368247_2022-09-26_1017AM.vpr3"
          snb_entity_id = url.split("/")[7]
          colscans = read_evipr(url, False)
          first_trace = get_vals(colscans, 0)  # the first well
          trace_size = first_trace.size
          traces = []
          well_labels = []
          rows = []
          cols = []
          frame_ids = []
          for well in range(0, 384):
              trace = get_vals(colscans, well, TraceType.NORM_RATIO)
              traces.append(list(trace))
              frame_ids.append(list(range(0, trace_size)))
              row, col = get_row_col(well)
              well_label = chr(65 + row) + str(col + 1)
              well_labels.append(well_label)
              rows.append(row)
              cols.append(col)

          # Need to create a tall-skinny table to trellis the results in Spotfire.
          # The rows of the tall-skinny need to be ordered by well number
          # so they distribute values sequentially across the trellis
          # This can be accomplished by zipping the 384 traces
          norm_ratios = zip(*traces)
          frames = zip(*frame_ids)
          # Chain the traces into a single list
          norm_ratio_col = list(itertools.chain.from_iterable(norm_ratios))
          frames_col = list(itertools.chain.from_iterable(frames))
          # Generate an order list of well numbers and well labels
          well_labels_col = well_labels * trace_size
          # prepare results as Spotfire Table
          frame_id_column = ColumnData(name=f'Frame ID', dataType=DataType.INTEGER, values=frames_col)
          ratio_column = ColumnData(name=f'Normal Ratio', dataType=DataType.FLOAT, values=norm_ratio_col)
          well_label_column = ColumnData(name=f'Well Label', dataType=DataType.STRING, values=well_labels_col)

          traces_datatable = TableData(tableName=f'{fileName}',
                                       columns=[frame_id_column, ratio_column, well_label_column])
          output_tables.append(traces_datatable)
          print(f"Generated EVIPR Traces Spotfire Datatable for {fileName}")
          # Peak Counting
          region_aggregate_values = calc_raw_region_aggregate_values(traces, 384,
                                                                     difference_method_type,
                                                                     difference_method_calculation,
                                                                     difference_method_num_points,
                                                                     peak_regions, difference_method)
          peak_counts = calc_peak_counts(region_aggregate_values, peak_regions,
                                         difference_method, threshold_computation_region,
                                         peak_count_normalization_region, scale_ref_well_type_indices,
                                         central_ref_well_type_indices, threshold_val)

          peak_counts_column = ColumnData(name=f'Raw Counts', dataType=DataType.INTEGER, values=peak_counts)
          well_label_col = ColumnData(name=f'Well Label', dataType=DataType.STRING, values=well_labels)
          row = ColumnData(name=f'Row', dataType=DataType.INTEGER, values=rows)
          col = ColumnData(name=f'Row', dataType=DataType.INTEGER, values=cols)

          peak_counts_datatable = TableData(tableName=f'Counts {fileName}',
                                            columns=[row, col, well_label_col, peak_counts_column])
          output_tables.append(peak_counts_datatable)
          print(f"Generated EVIPR Peak Counts Spotfire Datatable for {fileName}")

      response = DataFunctionResponse(outputTables=output_tables)
      print("Done")
      return response


  class TraceType(Enum):
      SHORT = 1
      LONG = 2
      RATIO = 3
      NORM_RATIO = 4
      STUPID_NORM_RATIO = 5


  def get_row_col(well_num):
      col = well_num % 24
      row = well_num // 24
      return row, col


  def get_vals(colscans, well_num, trace_type=TraceType.NORM_RATIO, norm_fraction=0.05):
      row, col = get_row_col(well_num)
      scan_num = col % 6  # which data block to read from
      block_num = col // 6

      # print row, col, scan_num, block_num
      well_offset = row * 2 + (block_num * 32)
      stride = 16 * 2 * 4  # 16 columns * 2 channels * 4 electrodes

      if trace_type == TraceType.SHORT:
          vals = (colscans[scan_num][well_offset::stride] * 1.0)
      elif trace_type == TraceType.LONG:
          vals = colscans[scan_num][well_offset + 1::stride]
      elif trace_type == TraceType.RATIO:
          vals = (colscans[scan_num][well_offset::stride] * 1.0) / colscans[scan_num][well_offset + 1::stride]
      elif trace_type == TraceType.NORM_RATIO:
          vals = (colscans[scan_num][well_offset::stride] * 1.0) / colscans[scan_num][well_offset + 1::stride]
          vals = vals / np.mean(vals[0:int(norm_fraction * len(vals))])
      elif trace_type == TraceType.STUPID_NORM_RATIO:
          # NOTE: this replicates a bug in the original version, which just normalizes by the first point instead of the first 5% of the data
          vals = (colscans[scan_num][well_offset::stride] * 1.0) / colscans[scan_num][well_offset + 1::stride]
          vals = vals / vals[0]
      return vals


  def read_evipr(fname, do_print=True):
      colscans = []
      dtype = np.dtype("<H")

      if isinstance(fname, tuple):
          import boto3
          from io import BytesIO
          s3 = boto3.client("s3")
          fil = BytesIO(s3.get_object(Bucket=fname[0], Key=fname[1])['Body'].read())
          fil.seek(0)
      elif fname.startswith("http"):
          req = Request(fname)
          req.add_header("x-api-key", snb_api_key)
          fil = io.BytesIO(urlopen(req).read())
      else:
          fil = open(fname, 'rb')

      # read header size and skip it
      siz = struct.unpack('i', fil.read(4))[0]
      if do_print:
          print("--> Header size:", siz, " Current file pointer: ", fil.tell())
          print(fil.read(siz))
          print("--> Current file pointer: ", fil.tell())
      else:
          fil.seek(siz, 1)

      for i in range(0, 6):
          # read protocol header size and skip it
          siz = struct.unpack('i', fil.read(4))[0]
          if do_print:
              print("--> PHeader size:", siz, " Current file pointer: ", fil.tell())
              print(fil.read(siz))
              print("--> Current file pointer: ", fil.tell())
          else:
              fil.seek(siz, 1)

          # read protocol size
          siz = struct.unpack('i', fil.read(4))[0]
          # here siz is the number of unsigned shorts to read, not the number of bytes
          if do_print:
              print("--> Protocol size", siz, " Current file pointer: ", fil.tell())
              # read protocol N
              num_scans = siz / 8 / 16
              print("--> Protocol %d size %d #timepoints: %f" % (i + 1, siz, num_scans))

          bytes = fil.read(siz * 2)  # read siz two-byte shorts
          colscan = np.frombuffer(bytes, dtype=dtype)
          # colscan = np.fromfile(fil, dtype=dtype, count=siz)
          colscans.append(colscan)

          if do_print:
              print("--> After reading block, current file pointer: ", fil.tell())

          # read post-protocol
          post_bytes = fil.read(4)
          siz = struct.unpack('i', post_bytes)[0]
          if do_print:
              print("--> Post Header size:", siz, " Current file pointer: ", fil.tell())
              print(fil.read(siz))
              print("---> Current file pointer: ", fil.tell())
          else:
              fil.seek(siz, 1)
      return colscans


  def read_traces(fname, do_print=True):
      # Load the plate
      colscans = read_evipr(fname, do_print)
      vals = get_vals(colscans, 0)  # the first well

      AllTraces = np.zeros((384, len(vals)))
      # Subtracting 1 is necessary for the Minkowski norm to work
      for nnn in range(0, 384):
          vals = get_vals(colscans, nnn)
          AllTraces[nnn, :] = vals - 1.0
      return AllTraces


  # This is a no-op for now as we don't have calculateFromAndToIndex
  def getRegion(region):
      rangeMin, rangeMax, method = region
      # rangeMin, rangeMax = calculateFromAndToIndex((rangeMin, rangeMax), isTimeBased(), measurementTimesInSeconds)
      return rangeMin + 1, rangeMax + 1, method


  # Not using GeneData StatsUtils safe methods
  def getAggregation(vals, method):
      if method.upper() == 'MAX':
          # agg = StatsUtils.safeMax(vals)
          agg = max(vals)
      elif method.upper() == 'MIN':
          # agg = StatsUtils.safeMin(vals)
          agg = min(vals)
      elif method.upper() == 'AVG':
          # agg = StatsUtils.mean(vals)
          agg = statistics.mean(vals)
      return agg


  def calc_raw_region_aggregate_values(traces, well_count, difference_method_type,
                                       difference_method_calculation, difference_method_num_points,
                                       peak_regions, difference_method):
      region_aggregate_values = [None] * well_count
      for wellNum in range(well_count):
          region_aggregate_values[wellNum] = {}
          for R in peak_regions:
              range_min, range_max, method = getRegion(peak_regions[R])
              region_aggregate_values[wellNum][R] = getAggregation(traces[wellNum][range_min:range_max], method)
              if difference_method:
                  # also calculate difference value for the region
                  if difference_method_type == 'pre':
                      region_aggregate_values[wellNum][R + "diff"] = getAggregation(
                          traces[wellNum][range_min - difference_method_num_points:range_min],
                          difference_method_calculation)
                  else:
                      region_aggregate_values[wellNum][R + "diff"] = getAggregation(
                          traces[wellNum][range_max:range_max + difference_method_num_points],
                          difference_method_calculation)
      return region_aggregate_values


  def calc_peak_counts(region_aggregate_values, peak_regions,
                       difference_method, threshold_computation_region,
                       peak_count_normalization_region, scale_reference_well_type_indices,
                       central_reference_well_type_indices, threshold_val):
      well_count = len(region_aggregate_values)
      if not difference_method:
          R = threshold_computation_region

          threshold_mean = statistics.mean(
              [region_aggregate_values[wellNum][threshold_computation_region] / region_aggregate_values[wellNum][
                  peak_count_normalization_region]
               for wellNum in scale_reference_well_type_indices])
          dmso_mean = statistics.mean(
              [region_aggregate_values[wellNum][threshold_computation_region] / region_aggregate_values[wellNum][
                  peak_count_normalization_region]
               for wellNum in central_reference_well_type_indices])
          cutoff = threshold_val * (threshold_mean - dmso_mean) / 100.0 + dmso_mean

          values = [0] * well_count
          for well in range(well_count):
              pc = 0
              for R in peak_regions:
                  if R != peak_count_normalization_region:
                      if region_aggregate_values[well][R] / region_aggregate_values[well][
                          peak_count_normalization_region] >= cutoff:
                          pc += 1
              values[well] = pc
      else:
          # NOTE: difference method.  This assumes input trace is normalized already
          threshold_mean = statistics.mean(
              [(region_aggregate_values[wellNum][threshold_computation_region] - region_aggregate_values[wellNum][
                  threshold_computation_region + "diff"]) + 1.0
               for wellNum in scale_reference_well_type_indices])
          dmso_mean = statistics.mean(
              [(region_aggregate_values[wellNum][threshold_computation_region] - region_aggregate_values[wellNum][
                  threshold_computation_region + "diff"]) + 1.0
               for wellNum in central_reference_well_type_indices])
          cutoff = threshold_val * (threshold_mean - dmso_mean) / 100.0 + dmso_mean

          values = [0] * well_count
          for well in range(well_count):
              pc = 0
              for R in peak_regions:
                  if R != peak_count_normalization_region:
                      if (region_aggregate_values[well][R] - region_aggregate_values[well][R + "diff"]) + 1.0 >= cutoff:
                          pc += 1
              values[well] = pc
      return values


  class PlateMapWell:
      def __init__(self, well_number, well_type):
          self.well_number = well_number
          self.well_type = well_type


  def make_plate_map(scale_reference_well_type, central_reference_well_type):
      plate_map = []
      for i in range(0, 384):
          well_type = "Compound"
          if i % 24 == 0:
              well_type = central_reference_well_type
          # elif i ==4 or (i+28) % 24 == 0:
          elif (i + 1) % 24 == 0:
              well_type = scale_reference_well_type
          plate_map.append((i, well_type))
      return plate_map
tags:
- color: '#dddddd'
  text: Vertex
ironPython: |
  import clr
  from Spotfire.Dxp.Application import PanelTypeIdentifiers
  from Spotfire.Dxp.Application.Visuals import ScatterPlot, LineChart, TrellisMode, IndividualScalingMode, AxisRange, \
      MarkerClass
  from System.Drawing import *  # Color
  from System import *  # ColorTranslator
  from System import *  # Collections
  from System import AppDomain

  for asm in AppDomain.CurrentDomain.GetAssemblies():
      if asm.GetName().Name == 'Charts':
          clr.AddReference(asm.FullName)

  from Charts import ChartsModel

  model_type = ChartsModel

  # tableId pass in args
  dataTables = ResultTables
  #dataTables = Document.Data.Tables
  tableNames = ['Trace KB_BC_VV368247', 'Count KB_BC_VV368247', 'Trace KB_BC_VV357314', 'Count KB_BC_VV357314']

  r = 0
  for dataTable in dataTables:
      r += 1
      #if r > 1 and r % 2 == 0:
      if r % 2 != 0:
          if not dataTable:
              raise Exception('target table not found')

          page = Document.Pages.AddNew(tableNames[r - 1])
          panelsToHide = [PanelTypeIdentifiers.DataPanel, PanelTypeIdentifiers.DetailsOnDemandPanel]

          for panel in page.Panels:
              if panel.TypeId in panelsToHide:
                  if panel.Visible:
                      panel.Visible = False

          filteringScheme = Document.FilteringSchemes[0]
          page.FilterPanel.FilteringSchemeReference = filteringScheme

          # add and configure LineChart
          lineChart = page.Visuals.AddNew[LineChart]()
          lineChart.Title = "EVIPR Normal Ratio " + tableNames[r - 1]
          lineChart.Data.DataTableReference = dataTable
          lineChart.XAxis.Expression = '<' + 'baserowid()' + '>'
          lineChart.XAxis.ManualZoom = True
          lineChart.XAxis.Scale.ShowGridlines = False
          lineChart.XAxis.Scale.ShowLabels = False
          lineChart.XAxis.ShowAxisSelector = False

          # build YAxis expression
          lineChart.YAxis.Expression = '[Normal Ratio]'
          lineChart.Trellis.TrellisMode = TrellisMode.Panels
          lineChart.Trellis.PanelAxis.Expression = '<[Well Label]>'
          lineChart.Trellis.ManualLayout = True
          lineChart.Trellis.ManualColumnCount = 24
          lineChart.Trellis.ManualRowCount = 16
          lineChart.YAxis.IndividualScaling = False
          lineChart.YAxis.IndividualScalingMode = IndividualScalingMode.Trellis
          # lineChart.YAxis.Range = AxisRange(0.85, 1.4)
          lineChart.YAxis.ShowAxisSelector = False
          color = ColorTranslator.FromHtml("#0eff00")
          lineChart.ColorAxis.Coloring.DefaultColor = color

          well_order = Collections.Generic.List[str](
              ["A1", "A2", "A3", "A4", "A5", "A6", "A7", "A8", "A9", "A10", "A11", "A12", "A13", "A14", "A15", "A16",
               "A17", "A18", "A19", "A20", "A21", "A22", "A23", "A24", "B1", "B2", "B3", "B4", "B5", "B6", "B7", "B8",
               "B9", "B10", "B11", "B12", "B13", "B14", "B15", "B16", "B17", "B18", "B19", "B20", "B21", "B22", "B23",
               "B24", "C1", "C2", "C3", "C4", "C5", "C6", "C7", "C8", "C9", "C10", "C11", "C12", "C13", "C14", "C15",
               "C16", "C17", "C18", "C19", "C20", "C21", "C22", "C23", "C24", "D1", "D2", "D3", "D4", "D5", "D6", "D7",
               "D8", "D9", "D10", "D11", "D12", "D13", "D14", "D15", "D16", "D17", "D18", "D19", "D20", "D21", "D22",
               "D23", "D24", "E1", "E2", "E3", "E4", "E5", "E6", "E7", "E8", "E9", "E10", "E11", "E12", "E13", "E14",
               "E15", "E16", "E17", "E18", "E19", "E20", "E21", "E22", "E23", "E24", "F1", "F2", "F3", "F4", "F5", "F6",
               "F7", "F8", "F9", "F10", "F11", "F12", "F13", "F14", "F15", "F16", "F17", "F18", "F19", "F20", "F21",
               "F22", "F23", "F24", "G1", "G2", "G3", "G4", "G5", "G6", "G7", "G8", "G9", "G10", "G11", "G12", "G13",
               "G14", "G15", "G16", "G17", "G18", "G19", "G20", "G21", "G22", "G23", "G24", "H1", "H2", "H3", "H4", "H5",
               "H6", "H7", "H8", "H9", "H10", "H11", "H12", "H13", "H14", "H15", "H16", "H17", "H18", "H19", "H20", "H21",
               "H22", "H23", "H24", "I1", "I2", "I3", "I4", "I5", "I6", "I7", "I8", "I9", "I10", "I11", "I12", "I13",
               "I14", "I15", "I16", "I17", "I18", "I19", "I20", "I21", "I22", "I23", "I24", "J1", "J2", "J3", "J4", "J5",
               "J6", "J7", "J8", "J9", "J10", "J11", "J12", "J13", "J14", "J15", "J16", "J17", "J18", "J19", "J20", "J21",
               "J22", "J23", "J24", "K1", "K2", "K3", "K4", "K5", "K6", "K7", "K8", "K9", "K10", "K11", "K12", "K13",
               "K14", "K15", "K16", "K17", "K18", "K19", "K20", "K21", "K22", "K23", "K24", "L1", "L2", "L3", "L4", "L5",
               "L6", "L7", "L8", "L9", "L10", "L11", "L12", "L13", "L14", "L15", "L16", "L17", "L18", "L19", "L20", "L21",
               "L22", "L23", "L24", "M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "M11", "M12", "M13",
               "M14", "M15", "M16", "M17", "M18", "M19", "M20", "M21", "M22", "M23", "M24", "N1", "N2", "N3", "N4", "N5",
               "N6", "N7", "N8", "N9", "N10", "N11", "N12", "N13", "N14", "N15", "N16", "N17", "N18", "N19", "N20", "N21",
               "N22", "N23", "N24", "O1", "O2", "O3", "O4", "O5", "O6", "O7", "O8", "O9", "O10", "O11", "O12", "O13",
               "O14", "O15", "O16", "O17", "O18", "O19", "O20", "O21", "O22", "O23", "O24", "P1", "P2", "P3", "P4", "P5",
               "P6", "P7", "P8", "P9", "P10", "P11", "P12", "P13", "P14", "P15", "P16", "P17", "P18", "P19", "P20", "P21",
               "P22", "P23", "P24"])
          dataTable.Columns["Well Label"].Properties.SetCustomSortOrder(well_order)
          for i in lineChart.Legend.Items:
              if i.Title == "Data table":
                  i.Visible = True
                  i.ShowTitle = True
              else:
                  i.Visible = False  # Hide any legend items not caught above

          page.AutoConfigure()

  # dataTable2 = ResultTables[1]
  # dataTable2 = Document.Data.Tables.Item['Script Output Table (2)']
  r = 0
  for dataTable2 in dataTables:
      r += 1
      #if r > 1 and r % 2 != 0:
      if r % 2 == 0:
          if not dataTable2:
              raise Exception('target table not found')

          page2 = Document.Pages.AddNew(tableNames[r - 1])
          panelsToHide = [PanelTypeIdentifiers.DataPanel, PanelTypeIdentifiers.DetailsOnDemandPanel]

          for panel in page2.Panels:
              if panel.TypeId in panelsToHide:
                  if panel.Visible:
                      panel.Visible = False

          filteringScheme = Document.FilteringSchemes[0]
          page2.FilterPanel.FilteringSchemeReference = filteringScheme

          # add and configure LineChart
          sp = page2.Visuals.AddNew[ScatterPlot]()
          sp.Title = "EVIPR Peak " + tableNames[r - 1]
          sp.Data.DataTableReference = dataTable2
          sp.XAxis.Expression = '[Row]'

          # build YAxis expression
          sp.YAxis.Expression = '[Row (2)]'

          sp.MarkerClass = MarkerClass().Tile
          sp.ColorAxis.Expression = "<[Raw Counts]>"
          page2.AutoConfigure()

  Document.ActivePageReference = Document.Pages[2]
allowedClients:
- Analyst
- WebPlayer
demoUrl: 
limitBy: none
